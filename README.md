# ENIGMA - Voice AI Detector ðŸ”ŠðŸ¤–

ENIGMA is a productionâ€‘style AI vs Human Voice Detection System that demonstrates a full pipeline from audio ingestion to machineâ€‘learning prediction using FastAPI, an audio preprocessing pipeline, and an automated n8n workflow.

Repository layout:

â”œâ”€â”€ n8n/
â”‚   â””â”€â”€ workflow.json          # n8n webhook â†’ FastAPI integration
â”‚
â”œâ”€â”€ ml-service/
â”‚   â”œâ”€â”€ api.py                 # FastAPI prediction service
â”‚   â”œâ”€â”€ audio_pipeline.py      # Base64 â†’ waveform processing
â”‚   â”œâ”€â”€ feature_extractor.py   # Audio feature extraction
â”‚   â”œâ”€â”€ train_model.py         # RandomForest training script
â”‚   â”œâ”€â”€ test_pipeline.py       # Audio pipeline validation
â”‚   â”œâ”€â”€ test_predict.py        # Batch prediction script
â”‚   â”œâ”€â”€ model.pkl              # Trained ML model
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ human/             # Human voice samples
â”‚       â””â”€â”€ ai/                # AIâ€‘generated voice samples
â”‚
â””â”€â”€ README.md

Overview:

This project simulates a realâ€‘world architecture where voice audio is received via webhook automation, processed through an audio normalization pipeline, transformed into numerical features, and classified using a trained RandomForest model.

Core Goals

Detect whether a voice is Human or AI Generated

Provide a secure API with authentication

Demonstrate scalable ML + Backend integration

Support automation workflows (n8n)



System Architecture:

Client / Tester / n8n
        â”‚
        â–¼
FastAPI (/process)
        â”‚
        â–¼
Base64 Decode
        â”‚
        â–¼
Audio Pipeline (pydub + librosa)
        â”‚
        â–¼
Feature Extraction
        â”‚
        â–¼
RandomForest Model
        â”‚
        â–¼
Prediction + Confidence


Replace `model.pkl` with your trained model and implement real feature extraction.

---

## â–¶ï¸ Quick start (local)
1. Install Python packages (recommended virtualenv):

   ```bash
   pip install numpy librosa scikit-learn pydub soundfile
   ```

2. Install FFmpeg and ensure it is on your PATH (required by `pydub`). See https://ffmpeg.org/.

3. Generate a model (if none exists):

   ```bash
   python ml-service/train_model.py
   ```

4. Run the demo prediction (uses an `.mp3` in `ml-service/data/` if present; otherwise falls back to a tiny WAV base64):

   ```bash
   python ml-service/test_predict.py
   ```

---

## ðŸ§ª Testing & expected behavior
- `test_pipeline.py` verifies Base64 decoding â†’ resample â†’ trim â†’ pad and prints array shape/duration.
- `test_predict.py` performs an in-memory prediction: it will look for `.mp3` in `ml-service/data/`, encode it to base64, run it through the pipeline + feature extractor, and return a prediction using `model.pkl`.

Notes:
- Feature extraction expects >= 1s input; pipeline pads to 3s by default.
- `model.pkl` must be present (run training if missing).
---

## ðŸš§ Known issues & TODO (prioritized)
1. Data collection: **Collect 200 human + 200 AI** voice samples. (Current: ~2/2) â€” Highest priority
2. Add evaluation: train/validation split, metrics (accuracy, precision, recall, ROC AUC), and threshold selection in `train_model.py` â€” Important
3. Implement API server (FastAPI recommended) with `/detect-voice` endpoint, JSON responses, and proper error handling â€” Important
4. Implement API key authentication server-side (workflow currently expects it) â€” Important
5. Add `requirements.txt`, CI (pytest), and GitHub Actions to run tests â€” Medium
6. Improve explainability and README documentation for decisions & thresholds â€” Medium

---

## ðŸ‘¥ Contributors
- Member 1 â€” ML model, feature extractor: **(status: feature extraction & RF training done; evaluation missing)**
- Member 2 â€” Audio pipeline: **(status: pipeline implemented; trimming & padding done; integration to API pending)**
- Member 3 â€” Backend/API: **(status: workflow references API, server missing)**
- Member 4 â€” Integration & docs: **(status: partial; README cleaned; explanation pending)**



---

## ðŸ¤ How to help / contribute
- Add more labeled audio files under `ml-service/data/human` and `ml-service/data/ai` (prefer `.wav` or `.mp3`) and name them consistently.
- Implement an API in `ml-service/` (FastAPI suggested) with an authenticated `/detect-voice` endpoint.
- Add proper train/validation code and a script to output evaluation metrics and a chosen decision threshold.
- Create `requirements.txt` and add CI with `pytest`.

---
hello, to work on just put the command
pip install fastapi uvicorn
python -m uvicorn api:app --reload --port 5000
u will get to see:
Uvicorn running on http://127.0.0.1:5000
Application startup complete
and next, go to browser and test
http://localhost:5000/docs
Swagger UI will open 
Next go to  Postman
there  POST
http://localhost:5678/webhook-test/process-audio
and next select body 
in that raw ->JSON->there u need to insert
{
  "audio_url": "https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3",
  "message": "testing"
}
then press send u will get the output:
{
    "status": "success",
    "prediction": "HUMAN",
    "confidence": 0.5,
    "sample_rate": 16000
}


